<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Spectral Clustering谱聚类 - 聚类算法]]></title>
      <url>%2F2017%2F04%2F20%2FSpectral-Clustering%E8%B0%B1%E8%81%9A%E7%B1%BB_-_%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[谱聚类算法建立在谱图理论基础上，与传统的聚类算法相比，它具有能在任意形状的样本空间上聚类且收敛于全局最优解的优点。该算法首先根据给定的样本数据集定义一个描述成对数据点相似度的亲合矩阵,并且计算矩阵的特征值和特征向量，然后选择合适的特征向量聚类不同的数据点。谱聚类算法建立在图论中的谱图理论基础上，其本质是将聚类问题转化为图的最优划分问题，是一种点对聚类算法。—-Baidu Baike 问题描述谱聚类（Spectral Clustering，下面简称SC）算法建立在图论中的谱图理论基础上，其本质是将聚类问题转化为图的最优划分问题，使子图内部尽量相似，子图间距离尽量远，以达到聚类的目的。 问题定义如下： 图定义 无向图$G=(V, E)$：$V$表示顶点集合，数目为$n$，$E$表示边的集合。 权重矩阵$W$：$n*n$的矩阵，值$w_{ij}$表示顶点$ij$之间边的权值，对于任意$w_{ij}=w_{ji}$，$w_{ii}=0$。 $G$中每个顶点表示一个样本，边的权值$w_{ij}$表示样本$ij$的相似性。 度矩阵$D$：$n*n$的对角矩阵，$D_{ii}$等于$W$中第i行所有元素的和，即跟样本$i$相连的边权重的和。 损失函数定义我们定义一个『截函数』的概念，当一个无向图被分割多个子图时，『截』就是指子图间的边的连接权重和。SC的目的是找到一种最优的分割，使得子图间的『截』最小，子图内权重最高。我们以二分类为例： $$Cut(G_1, G_2) = \sum_{i \in G_1, j \in G_2} w(i, j)$$ 下图是一个简单的例子，此例中$Cut = w_{15} + w_{34} = 0.3$。 Laplacian矩阵拉普拉斯矩阵，也称为基尔霍夫矩阵，是图的一种矩阵表示形式。定义为$L = D - W$。它的性质： Laplacian矩阵是对称半正定矩阵，特征值非负； Laplacian矩阵最小特征值是0，相应的特征向量$I$; 所有非零特征向量与$I$内积都未0，即正交； 对于任何实向量$f$，都有下面公式成立： $$f^TLf = \frac{1}{2} \sum^n_{i,j=1}w_{ij}(f_i-f_j)^2$$ 问题转换 我们首先定义$q_i$： $$ q_i = \begin{cases} c_1, &amp; i \in G_1\\ c_2, &amp; i \in G_2 \end{cases} $$ 其中$G_1$和$G_2$是两个子图。 将$Cut$变形： $$ Cut(G_1, G_2) = \sum_{i \in G_1, j \in G_2} w_{ij} = \frac {\sum^n_{i=1}\sum^n_{j=1}w_{ij}(q_i-q_j)^2}{2(c_1-c_2)^2} $$ 从公式可知，当且仅当$ij$属于不同子图时$(q_i-q_j)^2 / (c_1 - c_2)^2 = 1$，否则$(q_i-q_j)^2 / (c_1 - c_2)^2 = 0$。分母中有2，是因为每条边会被遍历两次。 分子变行： $$ \sum^n_{i=1}\sum^n_{j=1}w_{ij}(q_i-q_j)^2 = \sum^n_{i=1}\sum^n_{j=1}w_{ij}(q_i^2+q_j^2-2q_iq_j) \\ = \sum^n_{i=1}\sum^n_{j=1}-2w_{ij}q_iq_j + \sum^n_{i=1}\sum^n_{j=1}w_{ij}(q_i^2+q_j^2) \\ = -2\sum^n_{i=1}\sum^n_{j=1}w_{ij}q_iq_j + \sum^n_{i=1}\sum^n_{j=1}w_{ij}q_i^2 + \sum^n_{i=1}\sum^n_{j=1}w_{ij}q_j^2 \\ = -2\sum^n_{i=1}\sum^n_{j=1}w_{ij}q_iq_j + \sum^n_{i=1}2q_i^2(\sum^n_{j=1}w_{ij}) \\ = 2q^T(D - W)q \\ = 2q^TLq $$ $w_{ij}&gt;=0$，所以$q^TLq$对于$q$非0，都有$q^TLq&gt;=0$，所以$L$是半正定矩阵。 公式改写： $$ Cut(G_1, G_2) = \frac {q^TLq}{(c_1-c_2)^2} $$ 因此，求解最小损失函数$Cut$的问题转成求解最小化多项式$q^TLq$，求解$q$是离散问题，很难求解，我们将条件放宽到连续问题就可以利用瑞丽熵(Rayleigh quotient)的性质求解。 二类问题求解瑞丽熵(Rayleigh quotient)Rayleigh quotient定义如下： $$R(M, x) = \frac{x^TMx}{x^Tx}$$ 利用拉格朗日乘数法我们可以求解极值点问题，结合瑞丽熵： $$R(M, x) = x^TMx,\ \ s.t. x^Tx = const$$ $$L(x) = x^TMx - \lambda(x^Tx - const)$$ 对$L(x)$求导，寻找极值点： $$\frac{dL(x)}{dx} = 0 \\\Rightarrow 2Mx - 2\lambda x = 0\\\Rightarrow Mx = \lambda x$$ 所以，瑞丽熵公式极值点处的向量$x_1,x_2···x_n$就是$M$的特征向量，对应的极值就是$M$的特征值。 MINIMUM CUT方法要优化的目标就是上面定义的$Cut$。对于$c_1c_2$的取值并不影响分类结果，但会影响瑞丽熵的限制条件是否满足。为了方便，我们定义如下： $$q_i = \begin{cases} 1, &amp; i \in G_1\\-1, &amp; i \in G_2\end{cases}$$ 求解公式变为：$min\{q^TLq\},\ \ s.t. q^Tq=n \&amp; q^TI=0$。两个限制条件比较好理解，不做解释。此问题求解上面已经说明，最优的向量$q$为$L(x)$取最小特征值时，即0，对应特征向量$I$。当所有样本都归为一类时，无向图不进行截断，目标函数为0。但这样是无意义的，限制条件二把这种情况排除掉。因此我们要求解的是第二小的特征值对应的特征向量。 我们将求最小值问题转成求特征向量问题（幂法、雅可比法、QR方法）； 得到的$q_i$不再是初始定义的离散值，它只作为一个指示，通过阈值，将样本分割成两类。 问题：没有考虑孤立点的影响，如下图： 假如small cut是0.3，best cut是0.2+0.2，会导致图在small cut初切开，为了避免这样的想象，使得类别数量相对均衡，引入了Ratio Cut 方法。 RATIO CUT方法改写目标函数： $$RCut(G_1, G_2) = Cut(G_1, G_2) (\frac{1}{n_1}+\frac{1}{n_2}) = \frac {q^TLq}{(c_1-c_2)^2} (\frac{1}{n_1}+\frac{1}{n_2})$$ $n_1n_2$分别是属于两个类的样本数。ratio cut避免了上面minimum cut例子中的错误，不进考虑了子图间权重，也考虑了分割不均的情况。此时要转化成瑞丽熵问题，需要满足$q^Tq = const$，重新定义$q_i$： $$q_i = \begin{cases} \sqrt{\frac{n_2}{n_1n}}, &amp; i \in G_1\\-\sqrt{\frac{n_1}{n_1n}}, &amp; i \in G_2\end{cases}$$ $n_1 + n_2 = n$，所以上面定义满足瑞丽熵条件。优化问题转为：$min\{q^TLq\},\ \ s.t. q^Tq=1 \&amp; q^TI=0$。求解过程同上。 NORMALIZED CUT方法上面两种方法仅考虑子图外部的权重，Normalized Cut加入内部权重的衡量。目标函数如下： $$NCut(G_1, G_2) = Cut(G_1, G_2) (\frac{1}{d_1}+\frac{1}{d_2}) = \frac {q^TLq}{(c_1-c_2)^2} (\frac{1}{d_1}+\frac{1}{d_2})$$ $d_1$为$G_1$的内部点到所有点的边权重之和，$d_2$为$G_2$的内部点到其他所有点边权重之和，可知$d_1 + d_2 = d = ||D||$。 此时要转化成泛化的瑞丽熵问题（？），需要满足$q^TDq = const$，重新定义$q_i$： $$q_i = \begin{cases} \sqrt{\frac{d_2}{d_1d}}, &amp; i \in G_1\\-\sqrt{\frac{d_1}{d_1d}}, &amp; i \in G_2\end{cases}$$ 上面定义满足瑞丽熵条件。优化问题转为：$min\{q^TLq\},\ \ s.t. q^TDq=1 \&amp; q^TDI=0$。 利用拉格朗日乘数法求导结果跟之前（$Mx = \lambda x$）稍有不同： $$\frac{dLL(q)}{dq} = 0 \\\Rightarrow Lq - \lambda Dq = 0\\\Rightarrow Lq = \lambda Dq$$ 令:$L’= D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$，$q’ = D^{\frac{1}{2}}q$。因为$D$是对角矩阵，所以： $$D^{-\frac{1}{2}}D^{\frac{1}{2}} = 1\Rightarrow L’q’ = \lambda q’$$ 然后求解$L’$的特征值和特征矩阵即可，其中$L’$是归一化的拉普拉斯矩阵(Normalized Laplacian，对角线元素全为1) 。 多类问题求解与二聚类解法类似，只是不再是求解莫个特征向量，基本过程如下： 对于给定的数据构建图$G$，求图的度矩阵$D$和邻接矩阵$W$。 计算图的拉普拉斯矩阵$L$。 对$L$进行特征值分解，取前K小的特征值对应的特征向量，构成$n * k$的矩阵。 利用K-means聚类算法对新矩阵进行聚类。 将聚类问题转化成图分割问题，使聚类不在抽象化而具有了物理意思。像社交平台中的用户聚类、圈子聚类，通过谱聚类就可以很好的解释。与K-means相比，SC利用特征向量的元素表示样本特征，相当于是在理论基础下做降维操作，然后进行K-means聚类，计算量比K-means小。当处理高维数据，例如文本，尤为明显。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mxnet简单练习-MLP]]></title>
      <url>%2F2017%2F04%2F08%2Fmxnet%E7%AE%80%E5%8D%95%E7%BB%83%E4%B9%A0-MLP%2F</url>
      <content type="text"><![CDATA[mxnet是一个比较容易上手的DL框架，初级使用比较容易。但它的文档感觉不是很全面，想深入了解会比较困难。本文没什么技术含量，只是记录了一次简单的学习过程。 12import mxnet as mximport numpy as np 定义网络简单的MLP定义，包含两个全连接层，一个激活层，softmax作为输出。12345678910111213num_classes = 2# Create a place holder variable for the input datanet = mx.sym.Variable('data')# The first fully-connected layernet = mx.sym.FullyConnected(data=net, name='fc1', num_hidden=128)# Apply relu to the output of the first fully-connnected layernet = mx.sym.Activation(data=net, name='relu1', act_type="relu")# The second fully-connected layernet = mx.sym.FullyConnected(data=net, name='fc2', num_hidden=num_classes)# name must be softmax, or read https://github.com/dmlc/mxnet/issues/523# 这里如果没有指明label，名称必须是softmax，不然会因为找不到softmax_label而报错，报错信息可以查看上面的链接net = mx.sym.SoftmaxOutput(data=net, name='softmax')mx.viz.plot_network(net) mxnet可以方便的打印出网络结构图 构造数据1234# 随机生成20个样本a = np.random.uniform(-1, 1, 2000)# label 0 1b = [int(x) for x in np.random.uniform(0, 2, 200)] 大部分的mxnet的训练数据格式是data iterators，它类似于Python的iterator。通过不断调用next(),MXNet’s data iterator会返回一个batch的训练数据，或者StopIteration。详细的DataIter介绍会在单独的文章中介绍。 代码中的NDArrayIter方法返回一个迭代器，数据格式是mx.nd.NDArray或者numpy.ndarray。1234# 构造Data iteratorsbatch_size = 10num_features = 10train_iter = mx.io.NDArrayIter(np.array(a).reshape(200, num_features), np.array(b), batch_size, shuffle=True) 打印MLP中各层的纬度12345# 打印各层的纬度ex = net.simple_bind(ctx=mx.cpu(), data=(batch_size, num_features))args = dict(zip(net.list_arguments(), ex.arg_arrays))for name in args: print(name, args[name].shape) (&apos;fc2_weight&apos;, (2L, 128L)) (&apos;fc1_weight&apos;, (128L, 10L)) (&apos;softmax_label&apos;, (10L,)) (&apos;fc2_bias&apos;, (2L,)) (&apos;data&apos;, (10L, 10L)) (&apos;fc1_bias&apos;, (128L,)) 训练模型mxnet训练模型包含model和module两个库。下面是官网的介绍： Model: The model API provides a simplified way to train neural networks using common best practices. It’s a thin wrapper built on top of the ndarray and symbolic modules that make neural network training easy. Module: The module API, defined in the module (or simply mod) package, provides an intermediate and high-level interface for performing computation with a Symbol. One can roughly think a module is a machine which can execute a program defined by a Symbol. 从官网解释可以知道，Model是对ndarray的封装，使它更容易使用。module提供了更加高级的接口，我之后会单独一篇文章介绍两种模型。 123456789101112import logginglogging.getLogger().setLevel(logging.DEBUG)model = mx.model.FeedForward( symbol = net, # network structure num_epoch = 10, # number of data passes for training learning_rate = 0.1 # learning rate of SGD )model.fit( X=train_iter, # training data batch_end_callback = mx.callback.Speedometer(batch_size, 100) # output progress for each 200 data batches) INFO:root:Start training with [cpu(0)] INFO:root:Epoch[0] Resetting Data Iterator INFO:root:Epoch[0] Time cost=0.046 INFO:root:Epoch[1] Resetting Data Iterator INFO:root:Epoch[1] Time cost=0.077 INFO:root:Epoch[2] Resetting Data Iterator INFO:root:Epoch[2] Time cost=0.076 INFO:root:Epoch[3] Resetting Data Iterator INFO:root:Epoch[3] Time cost=0.126 INFO:root:Epoch[4] Resetting Data Iterator INFO:root:Epoch[4] Time cost=0.161 INFO:root:Epoch[5] Resetting Data Iterator INFO:root:Epoch[5] Time cost=0.081 INFO:root:Epoch[6] Resetting Data Iterator INFO:root:Epoch[6] Time cost=0.107 INFO:root:Epoch[7] Resetting Data Iterator INFO:root:Epoch[7] Time cost=0.040 INFO:root:Epoch[8] Resetting Data Iterator INFO:root:Epoch[8] Time cost=0.154 INFO:root:Epoch[9] Resetting Data Iterator INFO:root:Epoch[9] Time cost=0.086 模型预测和保存12model.predict(np.array(a[:10]).reshape(1, num_features))model.save("prefix_of_model") array([[ 0.41441375, 0.58558631]], dtype=float32)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hexo 搭建github pages记录]]></title>
      <url>%2F2017%2F04%2F07%2FHexo-%E6%90%AD%E5%BB%BAgithubpage%E8%AE%B0%E5%BD%95%2F</url>
      <content type="text"><![CDATA[之前在github上搭建过几次个人博客，使用过jekyll、hugo等。但jekyll操作起来确实麻烦，每次都要输入一系列命令，而且速度很慢。这次决定换掉jekyll，刚开始想用hugostaticgen。花了一下午时间，把网站搭建好，然后发现在使用Mathjax时，有问题。虽然官网给出了解决方案，但是一直没成功。所以只能放弃Hugo，转战hexo。在这里把hexo的搭建过程做个记录，方便之后查看。 准备工作本地搭建环境：mac软件包管理工具：Homebrew Git安装使用Github Page搭建博客, 需要在github建立仓库,仓库名为username.github.io。本地安装git，并配置密钥。具体过程略 Node安装123brew install node #最新版的node.js的包中已经集成了npm包管理工具node -v # 验证是否安装成功npm -v 如果机器没有安装过node，那么首先安装nvm。1brew install nvm 在shell的配置文件(~/.bash_profile)中添加如下内容：1234567891011121314export NVM_DIR=~/.nvmsource $(brew --prefix nvm)/nvm.sh``` 如果之前通过brew install node方式安装过node，那么需要先删除系统中存在的node：```bashbrew remove --force nodesudo rm -r /usr/local/lib/node_modulesbrew prunesudo rm -r /usr/local/include/node# 检查brew是否正常brew doctor Hexo安装与设置基本安装Node, npm和Git都安装成功, 开始安装hexo12npm install hexo -g #-g表示全局安装, npm默认为当前项目安装hexo version 如果遇到报错 123&#123; [Error: Cannot find module './build/Release/DTraceProviderBindings'] code: 'MODULE_NOT_FOUND' &#125;&#123; [Error: Cannot find module './build/default/DTraceProviderBindings'] code: 'MODULE_NOT_FOUND' &#125;&#123; [Error: Cannot find module './build/Debug/DTraceProviderBindings'] code: 'MODULE_NOT_FOUND' &#125; 则用下列语句安装1npm install hexo --no-optional 我在安装是也遇到报错，不过上面的命令没有起到作用，依然会报错，但是不影响编译。决题原因没有找到。 Hexo使用命令:123456789101112# clone自己git上的站点git clone git@github.com:zqg/zqg.github.io.gitcd zqg.github.io# hexo初始化hexo init#install before start bloggingnpm install# 自动根据当前目录下文件,生成静态网页hexo generate# 运行本地服务hexo server 浏览器输入http://localhost:4000就可以看到效果。 目录结构1234567891011.├── .deploy #需要部署的文件├── node_modules #Hexo插件├── public #生成的静态网页文件├── scaffolds #模板├── source #博客正文和其他源文件, 404 favicon CNAME 等都应该放在这里| ├── _drafts #草稿| └── _posts #文章├── themes #主题├── _config.yml #全局配置文件└── package.json 添加博文1hexo new "postName" #新建博文,其中postName是博文题目 博文会自动生成在博客目录下source/_posts\postName.md 注意不是hexo new page &quot;postName&quot;， 这个命令是添加二级菜单 文件自动生成格式:12345678title: "It Starts with iGaze: Visual Attention Driven Networkingwith Smart Glasses" #博文题目date: 2014-11-21 11:25:38 #生成时间tags: #标签, 多个标签也可以使用格式[tag1, tag2, tag3,...]- tag1- tag2- tag3categories: [cat1,cat2,cat3]--- 正文, 使用 Markdown 语法书写如果不想博文在首页全部显示, 并能出现阅读全文按钮效果, 需要在你想在首页显示的部分下添加1&lt;!--more--&gt; 主题更改12cd zqg.github.iogit clone https://github.com/iissnan/hexo-theme-next.git themes/next 在./_config.yml，修改主题为next1theme: next 查看本地效果12hexo ghexo s 部署到Git以上内容都是在本地进行查看, 现在将博客部署到github上123456#Githubdeploy: type: github repo: https://github.com/zqg/zqg.github.io.git branch: master # or gh-pages 注意事项:12345所有键的冒号后面留一个空格，如language: zh-CNurl不能为空,否则报错type: github报错hexo ERROR Deployer not found: github的解决方法：先运行 npm install hexo-deployer-git --save再改为 type: git 完整配置信息如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# Site #站点信息title: blog Name #标题subtitle: Subtitle #副标题description: my blog desc #描述author: me #作者language: zh-CN #语言timezone: Asia/Shanghai #时区# URLurl: http://yoururl.com #用于绑定域名, 其他的不需要配置root: /#permalink: :year/:month/:day/:title/permalink: posts/title.htmlpermalink_defaults:# Directory #目录source_dir: source #源文件public_dir: public #生成的网页文件tag_dir: tags #标签archive_dir: archives #归档category_dir: categories #分类code_dir: downloads/codei18n_dir: :lang #国际化skip_render:# Writing #写作new_post_name: :title.md #新文章标题default_layout: post #默认模板(post page photo draft)titlecase: false #标题转换成大写external_link: true #新标签页里打开连接filename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: #语法高亮 enable: true line_number: true #显示行号 auto_detect: true tab_replace:# Category &amp; Tag #分类和标签default_category: uncategorized #默认分类category_map:tag_map:# Date / Time format #日期时间格式## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination #分页per_page: 10 #每页文章数, 设置成 0 禁用分页pagination_dir: page# Extensions #插件和主题## 插件: http://hexo.io/plugins/## 主题: http://hexo.io/themes/theme: next# Deployment #部署, 发布在 GitHub 上面deploy:- type: git repo: git@github.com:username/username.github.io.git,master# Disqus #Disqus评论系统disqus_shortname: plugins: #插件，例如生成 RSS 和站点地图的- hexo-generator-feed- hexo-generator-sitemap 保存之后，便可以使用hexo d上传到Git。部署成功,使用username.git.io进行访问。 其他配置12345678910111213141516171819202122232425#站点建立时间since: 2014# 侧栏设置# post - 默认行为, 在文章页面(拥有目录列表)时显示# always - 在所有页面中都显示# hide - 在所有页面中都隐藏(可以手动展开)sidebar: post# 头像设置# avatar.png放置在/source/images/下# 或者采用网络地址avatar: /images/avatar.png# 菜单设置# 编辑主题配置文件themes/next/_config.yaml# 若站点运行在子目录中, 将链接前缀的 / 去掉menu: home: / archives: /archives categories: /categories tags: /tags commonweal: /404.html about: /about 标签页面添加一个标签云页面, 并在菜单中显示页面链接12# 新建tags页面hexo new page “tags” 1234567891011# 将页面的类型设置为 tagstitle: tagsdate: 2015-09-19 22:37:08type: "tags"comments: false---# 在菜单中添加链接。 编辑主题配置文件, 添加 tags 到 menu 中menu: tags: /tags 分类页面添加一个分类页面, 并在菜单中显示页面链接12# 新建 categories 页面hexo new page categories 123456789101112# 将页面的类型设置为categoriestitle: categoriesdate: 2015-09-19 22:38:00type: "categories"comments: false---#在菜单中添加链接. 编辑主题配置文件, 添加 categories 到 menu 中menu: categories: /categories 自定义页面以关于页面为例12新建一个 about 页面hexo new page "about" 编辑 source/about/index.md：123456789title: Aboutdate: 2014-11-1 11:11:11---# About Me# 菜单显示 about 链接, 主题配置文件中将 menu 中 about 前面的注释去掉menu: about: /about 解决Mathjax转义问题参考链接 segmentfault我的具体操作：12345678910111213# 打开nodes_modules/marked/lib/marked.js:# 去掉\的转义:# 找到下面的代码escape: /^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/,# 改为:escape: /^\\([`*&#123;&#125;\[\]()# +\-.!_&gt;])/,# 去掉_的斜体含义：# 找到em的符号em: /^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,# 改为:em:/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ltr From Ranknet To Lambdarank To Lambdamart]]></title>
      <url>%2F2017%2F02%2F07%2FLTR-From-RankNet-to-LambdaRank-to-LambdaMART%2F</url>
      <content type="text"><![CDATA[LTR 算法通常有三种手段，分别是：Pointwise、Pairwise 和 Listwise。Pointwise 和 Pairwise 类型的LTR算法，将排序问题转化为回归、分类或者有序分类问题。Listwise 类型的 LTR 算法则另辟蹊径，将用户查询（Query）所得的结果作为整体，作为训练用的实例（Instance）。 PointwisePointwis方法的主要思想是将排序问题转化为多类分类问题或者回归问题。假设对于查询query，与其相关的文档集合为：{d1, d2, …, dn}。那么: 多类分类：将query与di之间的相关度的程度作为label，一般的label等级划分方式为：｛Perfect, Excellent, Good, Fair, Bad｝一共五个类别。于是，对于一个查询及其文档集，可以形成n个训练实例。有了训练实例，我们可以使用任一种多类分类器进行学习，比如最大熵、SVM。 回归：将query与di之间的相关度作为value，利用regression model来得到一个query与document之间相关度的预测。 缺点： Pointwise完全从单文档的分类角度计算，没有考虑文档之间的相对顺序。而且它假设相关度是查询无关的，只要（query，di）的相关度相同，那么他们就被划分到同一个级别中，属于同一类。然而实际上，相关度的相对性是和查询相关的，比如一个常见的查询它会有很多相关的文档，该查询和它相关性相对靠后的文档的label标注级别时可能会比一个稀有的查询和它为数不多的高度相关文档的label标准级别更高。这样就导致训练样本的不一致，并且对于预测为同一label级别的文档之间也无法相对排序。 PairwisePairwise方法是目前比较流行的方法，效果也非常不错。它的主要思想是将Ranking问题形式化为二元分类问题。常用的机器学习的方法比较多，比如Boost、SVM、神经网络等。 对于同一query的相关文档集中，对任何两个不同label的文档，都可以得到一个训练实例（di,dj），如果di&gt;dj则赋值+1，反之-1，于是我们就得到了二元分类器训练所需的训练样本了，如下图所示。 测试时，只要对所有pair进行分类就可以得到所有文档的一个偏序关系，从而实现排序。 缺点 ： 尽管Pairwise对Pointwise做了改进，但该方法还是存在明显的问题 只考虑了两篇文档的相对顺序，没有考虑他们出现在搜索结果列表中的位置。排在前面的文档更为重要，如果出现在前面的文档判断错误，惩罚函数要明显高于排在后面判断错误。因此需要引入位置因素，每个文档对根据其在结果列表中的位置具有不同的权重，越排在前面权重越大，如果排错顺序其受到的惩罚也越大。 对于不同的查询相关文档集的数量差异很大，转换为文档对后，有的查询可能只有十几个文档对，而有的查询可能会有数百个对应的文档对，这对学习系统的效果评价带来了偏置。假设查询1对应500个文档对，查询2对应10个文档对，假设机器学习系统对应查询1能够判断正确480个文档对，对应查询2能够判断正确2个。对于总的文档对该系统准确率是（480+2）/（500+10）=95%，但从查询的角度，两个查询对应的准确率分别为：96%和20%，平均为58%，与总的文档对判断准确率相差巨大，这将使得模型偏向于相关文档集大的查询。Pairwise有很多的实现，比如Ranking SVM，RankNet，Frank，RankBoost等。 ListwiseListwise与上述两种方法不同，它是将每个查询对应的所有搜索结果列表作为一个训练样例。Listwise根据训练样例训练得到最优评分函数F，对应新的查询，评分F对每个文档打分，然后根据得分由高到低排序，即为最终的排序结果。目前主要有两种优化方法： 直接针对Ranking评价指标进行优化。比如常用的MAP, NDCG（下面介绍）。这个想法非常自然，但是往往难以实现，因为MAP、NDCG这样的评价指标通常是非平滑（连续）的，而通用的目标函数优化方法针对的都是连续函数。 优化损失函数 loss function。比如LambdaRank、LambdaMART。 From RankNet to LambdaRank to LambdaMARTLambdaMART 是一种 Listwise 类型的 LTR 算法，它基于 LambdaRank 算法和 MART (Multiple Additive Regression Tree) 算法，将搜索引擎结果排序问题转化为回归决策树问题。MART 实际就是梯度提升决策树（GBDT, Gradient Boosting Decision Tree）算法。GBDT 的核心思想是在不断的迭代中，新一轮迭代产生的回归决策树模型拟合损失函数的梯度，最终将所有的回归决策树叠加得到最终的模型。LambdaMART 使用一个特殊的 Lambda 值来代替上述梯度，也就是将 LambdaRank 算法与 MART 算法加和起来。考虑到 LambdaRank 是基于 RankNet 算法的，所以在搞清楚 LambdaMART 算法之前，我们首先需要了解 MART、RankNet 和 LambdaRank 是怎么回事。 RankNetRankNet是2005年微软提出的一种pairwise的Learning to rank算法，它从概率的角度来解决排序问题。RankNet提出了一种概率损失函数来学习Ranking Function，并应用Ranking Function对文档进行排序。这里的Ranking Function可以是任意对参数可导的模型，也就是说，该概率损失函数并不依赖于特定的机器学习模型，在论文中，RankNet是基于神经网络实现的。除此之外，GDBT等模型也可以应用于该框架。 学习过程变量定义：映射函数：$f(x_i)$ 样本i的估计值：$s_i = f(x_i)$ 样本ij的差值：$s_{ij} = f(x_i)-f(x_j)$ 目标差值：$S_{ij} \in \{-1,0,1\}$ # 1表示样本i排在j之前，0表示相同，-1表示j在前 对同一query的两个样本，经过预测得到两个分值$s_i s_j$，然后将他们映射成概率： i排序在j之前的目标概率：$\\\\overline{P}_{ij} = \frac{1}{2}(1+S_{ij}) \in \{0, 0.5, 1\}$ i排序在j之前的估计概率：$P_{ij}=\frac{1}{1+e^{-\sigma (s_i-s_j)}},\sigma$决定sigmoid形状，对最终结果影响不大。 交叉熵损失函数：$C_{ij}$ $$C_{ij} = C(s_{ij}) = -\overline{P}_{ij}logP_{ij}-(1-\overline{P}_{ij})log(1-P_{j})$$ $$\Rightarrow C_{ij}=\frac{1}{2}(1-S_{ij})\sigma(s_i-s_j)+ log(1+e^{-\sigma s_{ij}})$$ 由$C_{ij}$公式我们可以得出 ，当 $\overline{P}_{ij}=0.5$时，$C_{ij}$是以y轴对称的抛物线函数。 Learning to rank with two-layer Nerual Nets 两层神经网络梯度推导： $$s_i = g^3(\sum_jw_{ij}^{32}g^2(\sum_kw_{jk}^{21}x_k+b_j^2)+b_i^3) = g_i^3$$ $$\Rightarrow \frac{\partial{f}}{\partial{b_i^3}} =\frac{\partial{f}}{\partial{s_i}}g_i^{\prime3}=\Delta_i^3$$ $$\Rightarrow \frac{\partial{f}}{\partial{w_{in}^{32}}} =\frac{\partial{f}}{\partial{s_i}}g_i^{\prime3}g_n^2=\Delta_i^3g_n^2$$ $$\Rightarrow \frac{\partial{f}}{\partial{b_m^2}} =(\sum_i\frac{\partial{f}}{\partial{s_i}}g_i^{\prime3}w_{im^{32}})g_m^{\prime2}=\Delta_m^2$$ $$ \Rightarrow \frac{\partial{f}}{\partial{w_{mn}^{21}}} =x_n\Delta_m^2$$ RankNet中输出节点只有一个，即$i==0$。损失函数变成两个样本输出值的函数$f(s_2-s_1)$。 $$\frac{\partial{f}}{\partial{b^3}}=(\frac{\partial{s_2}}{\partial{\alpha}} - \frac{\partial{s_1}}{\partial{\alpha}})f^{\prime}$$ $$ \Rightarrow \frac{\partial{f}}{\partial{b^3}} =f^{\prime}(g_2^{\prime{3}}-g_1^{\prime3})=\Delta_2^3-\Delta_1^3$$ $$ \Rightarrow \frac{\partial{f}}{\partial{w_{n}^{32}}} =\Delta_2^3g_{2n}^2- \Delta_1^3g_{1n}^2$$ $$ \Rightarrow \frac{\partial{f}}{\partial{b_m^2}} =\Delta_2^3w_m^{32}g_{2m}^{\prime2} - \Delta_1^3w_m^{32}g_{1m}^{\prime2}$$ $$ \Rightarrow \frac{\partial{f}}{\partial{w_{mn}^{21}}} =\Delta_{2m}^2g_{2n}^1 - \Delta_{1m}^2g_{1n}^1$$ 一般的梯度下降法推导用梯度下降法求解是RankNet的关键思想，不仅可以用神经网络模型，GBDT也可以（xgboost、LightGBM），只不过是需要将$\frac{\partial C}{\partial w_k}$的求解，变成$\frac{\partial C}{\partial s_i}$。下面讲一下一般的梯度下降法的推导。 $$w_k\rightarrow w_k-\eta \frac{\partial C}{\partial w_k}$$ $\eta$为步长，损失函数的改变： $$\Delta C=\sum_k \frac{\partial C}{\partial w_k}\Delta w_k=\sum_k \frac{\partial C}{\partial w_k}(-\eta \frac{\partial C}{\partial w_k})=-\eta \sum_k (\frac{\partial C}{\partial w_k})^2 &lt; 0$$ 损失函数C是逐步减小的，我们继续拆分： $$\frac{\partial C}{\partial s_i} = \sigma (\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma (s_i-_j)}})=-\frac{\partial C}{\partial s_j}$$ $$\frac{\partial C}{\partial w_k}=\frac{\partial C}{\partial s_i}\frac{\partial s_i}{\partial w_k}+\frac{\partial C}{\partial s_j}\frac{\partial s_j}{\partial w_k}=\sigma (\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma (s_i-_j)}})(\frac{\partial s_i}{\partial w_k} -\frac{\partial s_j}{\partial w_k})$$ RankNet训练加速上面的学习过程中，对每一个样本对ij就要做一次参数更新，效率不是很高，比如采用BP神经网络模型，训练会非常慢。下面讲解一下mini-batch的方式更新w，这也是LambdaRank的基础。我们先定义 $\lambda$： $$\lambda_{ij} = \sigma (\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma (s_i-_j)}})=\frac{\partial C}{\partial s_i}$$ $$\Rightarrow \frac{\partial C}{\partial w_k}=\lambda_{ij} (\frac{\partial s_i}{\partial w_k} -\frac{\partial s_j}{\partial w_k})$$ 我们选取一个样本对集合$I$，为了使每个样本对$(i,j)$（$ij$必须属于同一个query）在集合中只出现一次，我们规定$(i,j)$中i要排在j之前，及$S_{ij}=1$。于是我们将$w_k$的更新值变成累加的形式： $$\Delta w_k=-\eta \sum_{i,j\in I}(\lambda_{ij}\frac{\partial{s_i}}{\partial{w_k}} - \lambda_{ij}\frac{\partial{s_j}}{\partial{w_k}})= -\eta \sum_{i}(\sum_{i&gt;j}\lambda_{ij}\frac{\partial{s_i}}{\partial{w_k}} - \sum_{k&gt;i} \lambda_{k,i}\frac{\partial{s_i}}{\partial{w_k}})=-\eta \sum_{i}\lambda_{i}\frac{\partial{s_i}}{\partial{w_k}}$$ 其中$\lambda_i$定义如下： 对样本i，我们找出所有排在它之前的样本组成$k,i\in I$，然后找到所有排在其后的样本组成$i,j\in I$。 $$\lambda_i = \sum_{i,j \in I} \lambda_{i,j} - \sum_{k,i\in I} \lambda_{k,i}$$ 经过上面的推导，就可以用mini-batch的方式更新$w$权重了。 LambdaRank上面介绍的RankNet是以error pair最少为优化目标，然而许多时候仅以error pair数来评价排序的好坏是不够的，因为它只关心pair的相对位置，不关心top K的结果，而IR排序中top K的结果是很重要的，以下图为例： 图中每一条短横线表示一个URL，蓝色表示与Query相关的URL，灰色表示不相关的URL。下面我们用error pair和NDCG分别来评估左右两个排序的好坏： Error pair指标对于排序1，排序错误的pair共13对，故cost=13。对于排序2，排序错误的pair共11对，故cost=11。 所以，从Error pair角度考虑，排序2要优于排序1 NDCG指标假设相关url分值为1，不相关为0。排序1与排序2具有相同的maxDCG@16,$maxDCG@16=\frac{1}{log(1+1)} + \frac{1}{log(1+2)}=1.63$对排序1，有$DCG@16=\frac{1}{log(1+1)}+\frac{1}{log(1+15)}=1.25$$NDCG@16=1.25/1.63=0.767$对排序2，有$DCG@16=\frac{1}{log(1+4)}+\frac{1}{log(1+10)}=0.72$$NDCG@16=0.72/1.63=0.442$ 所以，从NDCG指标来看，排序1要优于排序2。 从上面的计算可以看出，用error pair评估的结果有时不是我们期望的。与error pair不同，NDCG评估会强调top K的结果，这是符合我们预期的。而且，排序2中右侧的黑色箭头表示RankNet下一步梯度更新方向，长短表示强度。然而我们真正希望的是红色箭头的形式，排在前面的更新权重更大。 那我们可不可以用RankNet的思路优化像NDCG这样的不平滑、不连续的指标呢？答案是肯定的。LambdaRank的一个关键思想是，我们没有必要构建cost function本身，我们只需要知道梯度就可以了。所以LambdaRank算法分析排序问题，然后直接对RankNet里的$\lambda_{ij}$进行了修改。 $$\lambda_{ij}=\frac{-\sigma}{1+e^{\sigma (s_i-s_j)}}|\Delta Z_{ij}|$$ 式中$\Delta Z_{ij}$表示，将$i$和$j$交换位置后，待优化指标的变化，如$\Delta NDCG$就表示将$i$和$j$进行交换（其他url保持不变），交换后排序的NDCG与交换前排序的NDCG的差值。乘积前半部分代表了文档下一次迭代优化的方向和强度，由于引入了IR评价指标，Lambda梯度更关注位置靠前的高匹配度url的排序位置的提升。我们把改进后的算法称之为LambdaRank。 现在我们的优化目标不再是损失函数$C$最小，而是评估指标最大。所以权重更新方程变成： $$w_k\rightarrow w_k+\eta \frac{\partial C}{\partial w_k}$$ $$\Delta C=\frac{\partial C}{\partial w_k}\Delta w_k=\eta \sum_k (\frac{\partial C}{\partial w_k})^2 &gt; 0$$ LambdaRank相比RankNet的优势在于考虑了评价指标，直接对问题求解，所以效果更好。有效的避免了下调相关度高的url的位置这种情况的发生。 LambdaMARTLambdaRank中通过直接定义梯度，解决了引入评估指标带来的不连续的问题。这个Lambda梯度可以应用于任何使用梯度下降法求解的模型。Boosted tree是一种很灵活的模型，将它与Lambda相结合就是微软2009年发表的LambdaMART算法。 在MART（GBDT）中，当采用平方误差损失函数时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：残差 = 真实值 - 预测值 。对于一般的损失函数，Friedman提出用损失函数的负梯度值，作为残差的拟合值。 LambdaMART算法中，这个负梯度就是$\lambda_i$。 $$\lambda_{ij}=\frac{-\sigma |\Delta Z_{ij}|}{1+e^{\sigma (s_i-s_j)}}$$ 算法[Algorithm 1]中，Step 1-3 是用现有模型初始化初值。Step 6 计算每篇文档的$\lambda$梯度$y_i$。Step 7 计算每篇文档的二阶导数$w_i$。Step 9 是利用所有文档及它们的残差（$\lambda$）建立回归树。Step 11 是利用牛顿-拉夫森方法计算叶结点的权值。Step 14 是更新每篇文档的分数。 公开的数据集可以使用 LETOR Microsoft Learning to Rank Dataset Yahoo Learning to Rank Challenge]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MXNet设计和实现简介]]></title>
      <url>%2F2016%2F12%2F30%2FMXNet%E8%AE%BE%E8%AE%A1%E5%92%8C%E5%AE%9E%E7%8E%B0%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"><![CDATA[本文转自李牧子github上发表的一篇文章，对mxnet做了总体的介绍，搬运过来以作记录。 基础介绍(转) 神经网络本质上是一种语言，我们通过它来表达对应用问题的理解。例如我们用卷积层来表达空间相关性，RNN来表达时间连续性。根据问题的复杂性和信息如何从输入到输出一步步提取，我们将不同大小的层按一定原则连接起来。近年来随着数据的激增和计算能力的大幅提升，神经网络也变得越来越深和大。例如最近几次imagnet竞赛的冠军都使用有数十至百层的网络。对于这一类神经网络我们通常称之为深度学习。从应用的角度而言，对深度学习最重要的是如何方便的表述神经网络，以及如何快速训练得到模型。 命令式编程 &amp; 声明式编程对于一个优秀的深度学习系统，或者更广来说优秀的科学计算系统，最重要的编程接口的设计。他们都采用将一个_领域特定语言(domain specific language)嵌入到一个主语言中。例如numpy将矩阵运算嵌入到python中。这类嵌入一般分为两种，其中一种嵌入的较浅，其中每个语句都按原来的意思执行，且通常采用_命令式编程(imperative programming)，其中numpy和Torch就是属于这种。而另一种则用一种深的嵌入方式，提供一整套针对具体应用的迷你语言。这一种通常使用_声明式语言(declarative programing)_，既用户只需要声明要做什么，而具体执行则由系统完成。这类系统包括Caffe，theano和刚公布的TensorFlow。这两种方式各有利弊，总结如下 table th:first-of-type { width: 100px; } table tr:hover { background: #efefef; } 浅嵌入，命令式编程 深嵌入，声明式编程 如何执行 a=b+1 需要b已经被赋值。立即执行加法，将结果保存在a中。 返回对应的计算图(computation graph)，我们可以之后对b进行赋值，然后再执行加法运算 优点 语义上容易理解，灵活，可以精确控制行为。通常可以无缝的和主语言交互，方便的利用主语言的各类算法，工具包，bug和性能调试器。 在真正开始计算的时候已经拿到了整个计算图，所以我们可以做一系列优化来提升性能。实现辅助函数也容易，例如对任何计算图都提供forward和backward函数，对计算图进行可视化，将图保存到硬盘和从硬盘读取。 缺点 实现统一的辅助函数困和提供整体优化都很困难。 很多主语言的特性都用不上。某些在主语言中实现简单，但在这里却经常麻烦，例如if-else语句 。debug也不容易，例如监视一个复杂的计算图中的某个节点的中间结果并不简单。 不同框架编程模式比较目前现有的系统大部分都采用上两种编程模式的一种。与它们不同的是，MXNet尝试将两种模式无缝的结合起来。在命令式编程上MXNet提供张量运算，而声明式编程中MXNet支持符号表达式。用户可以自由的混合它们来快速实现自己的想法。例如我们可以用声明式编程来描述神经网络，并利用系统提供的自动求导来训练模型。另一方便，模型的迭代训练和更新模型法则中可能涉及大量的控制逻辑，因此我们可以用命令式编程来实现。同时我们用它来进行方便的调式和与主语言交互数据。下表我们比较MXNet和其他流行的深度学习系统 主语言 从语言 硬件 分布式 命令式 声明式 Caffe C++ Python/Matlab CPU/GPU n n y Torch Lua - CPU/GPU/FPGA n y n Theano Python - CPU/GPU n n y TensorFlow C++ Python CPU/GPU/Mobile y n y MXNet C++ Python/R/Julia/Go CPU/GPU/Mobile y y y （注，TensforFlow暂时没有公开其分布式实现） MXNet的系统架构MXNet的系统架构如下图所示：从上到下分别为各种主语言的嵌入，编程接口（矩阵运算，符号表达式，分布式通讯），两种编程模式的统一系统实现，以及各硬件的支持。接下一章我们将介绍编程接口，然后下一章介绍系统实现。之后我们给出一些实验对比结果，以及讨论MXNet的未来。 编程接口Symbol： 声明式的符号表达式MXNet使用多值输出的符号表达式来声明计算图。符号是由操作子构建而来。一个操作子可以是一个简单的矩阵运算“+”，也可以是一个复杂的神经网络里面的层，例如卷积层。一个操作子可以有多个输入变量和多个输出变量，还可以有内部状态变量。一个变量既可以是自由的，我们可以之后对其赋值；也可以是某个操作子的输出。例如下面的代码中我们使用Julia来定义一个多层感知机，它由一个代表输入数据的自由变量，和多个神经网络层串联而成。12345678data = mx.symbol.Variable('data')# first fullcfc1 = mx.symbol.FullyConnected(data=data, num_hidden=64)relu3 = mx.symbol.Activation(data=fc1, act_type="relu")# second fullcfc2 = mx.symbol.FullyConnected(data=relu3, num_hidden=10)# lossnet = mx.symbol.SoftmaxOutput(data=fc2, name='softmax') 在执行一个符号表达式前，我们需要对所有的自由变量进行赋值。上例中，我们需要给定数据，和各个层里隐式定义的输入，例如全连接层的权重和偏值。我们同时要申明所需要的输出，例如softmax的输出。 除了执行获得softmax输出外（通常也叫forward），符号表达式也支持自动求导来获取各权重和偏值对应的梯度（也称之为backward）。此外，我们还可以提前估计计算时需要的内存，符号表达式的可视化，读入和输出等。 NDArray：命令式的张量计算MXNet提供命令式的张量计算来桥接主语言的和符号表达式。下面代码中，我们在GPU上计算矩阵和常量的乘法，并使用numpy来打印结果12345&gt;&gt;&gt; import MXNet as mx&gt;&gt;&gt; a = mx.nd.ones((2, 3), mx.gpu())&gt;&gt;&gt; print (a * 2).asnumpy()[[ 2. 2. 2.] [ 2. 2. 2.]] 另一方面，NDArray可以无缝和符号表达式进行对接。假设我们使用Symbol定义了一个神经网络，那么我们可以如下实现一个梯度下降算法 12345for (int i = 0; i &lt; max_iter; ++i) &#123; network.forward(); network.backward(); network.weight -= eta * network.gradient&#125; 这里梯度由Symbol计算而得。Symbol的输出结果均表示成NDArray，我们可以通过NDArray提供的张量计算来更新权重。此外，我们还利用了主语言的for循环来进行迭代，学习率eta也是在主语言中进行修改。 上面的混合实现跟使用纯符号表达式实现的性能相差无二，然后后者在表达控制逻辑时会更加复杂。其原因是NDArray的执行会和Symbol类似的构建一个计算图，并与其他运算一同交由后台引擎执行。对于运算-=由于我们只是将其结果交给另一个Symbol的forward作为输入，因此我们不需要立即得到结果。当上面的for循环结束时，我们只是将数个Symbol和NDarray对应的计算图提交给了后台引擎。当我们最终需要结果的时候，例如将weight复制到主语言中或者保存到磁盘时，程序才会被阻塞直到所有计算完成。 KVStore：多设备间的数据交互MXNet提供一个分布式的key-value存储来进行数据交换。它主要有两个函数，push： 将key-value对从一个设备push进存储pull：将某个key上的值从存储中pull出来此外，KVStore还接受自定义的更新函数来控制收到的值如何写入到存储中。最后KVStore提供数种包含最终一致性模型和顺序一致性模型在内的数据一致性模型。在下面例子中，我们将前面的梯度下降算法改成分布式梯度下降。12345678910KVStore kvstore("dist_async");kvstore.set_updater([](NDArray weight, NDArray gradient) &#123; weight -= eta * gradient; &#125;);for (int i = 0; i &lt; max_iter; ++i) &#123; kvstore.pull(network.weight); network.forward(); network.backward(); kvstore.push(network.gradient);&#125; 在这里我们先使用最终一致性模型创建一个kvstore，然后将更新函数注册进去。在每轮迭代前，每个计算节点先将最新的权重pull回来，之后将计算的得到的梯度push出去。kvstore将会利用更新函数来使用收到的梯度更新其所存储的权重。 这里push和pull跟NDArray一样使用了延后计算的技术。它们只是将对应的操作提交给后台引擎，而引擎则调度实际的数据交互。所以上述的实现跟我们使用纯符号实现的性能相差无几。 读入数据模块数据读取在整体系统性能上占重要地位。MXNet提供工具能将任意大小的样本压缩打包成单个或者数个文件来加速顺序和随机读取。 通常数据存在本地磁盘或者远端的分布式文件系统上（例如HDFS或者Amazon S3)，每次我们只需要将当前需要的数据读进内存。MXNet提供迭代器可以按块读取不同格式的文件。迭代器使用多线程来解码数据，并使用多线程预读取来隐藏文件读取的开销。 训练模块MXNet实现了常用的优化算法来训练模型。用户只需要提供数据数据迭代器和神经网络的Symbol便可。此外，用户可以提供额外的KVStore来进行分布式的训练。例如下面代码使用分布式异步SGD来训练一个模型，其中每个计算节点使用两快GPU。 1234567891011121314import MXNet as mxmodel = mx.model.FeedForward( ctx = [mx.gpu(0), mx.gpu(1)], symbol = network, num_epoch = 100, learning_rate = 0.01, momentum = 0.9, wd = 0.00001, initializer = mx.init.Xavier(factor_type="in", magnitude=2.34))model.fit( X = train_iter, eval_data = val_iter, kvstore = mx.kvstore.create('dist_async'), epoch_end_callback = mx.callback.do_checkpoint('model_')) 系统实现计算图一个已经赋值的符号表达式可以表示成一个计算图。下图是之前定义的多层感知机的部分计算图，包含forward和backward。 其中圆表示变量，方框表示操作子，箭头表示数据依赖关系。在执行之前，MXNet会对计算图进行优化，以及为所有变量提前申请空间。 计算图优化计算图优化已经在数据库等领域被研究多年，我们目前只探索了数个简单的方法。注意到我们提前申明了哪些输出变量是需要的，这样我们只需要计算这些输出需要的操作。例如，在预测时我们不需要计算梯度，所以整个backforward图都可以忽略。而在特征抽取中，我们可能只需要某些中间层的输出，从而可以忽略掉后面的计算。 我们可以合并某些操作。例如 _a_b+1*只需要一个blas或者cuda函数即可，而不需要将其表示成两个操作。 我们实现了一些“大”操作，例如一个卷积层就只需要一个操作子。这样我们可以大大减小计算图的大小，并且方便手动的对这个操作进行优化。 内存申请内存通常是一个重要的瓶颈，尤其是对GPU和智能设备而言。而神经网络计算时通常需要大量的临时空间，例如每个层的输入和输出变量。对每个变量都申请一段独立的空间会带来高额的内存开销。幸运的是，我们可以从计算图推断出所有变量的生存期，就是这个变量从创建到最后被使用的时间段，从而可以对两个不交叉的变量重复使用同一内存空间。这个问题在诸多领域，例如编译器的寄存器分配上，有过研究。然而最优的分配算法需要 O(n2) 时间复杂度，这里n是图中变量的个数。MXNet提供了两个启发式的策略，每个策略都是线性的复杂度。 inplace, 在这个策略里，我们模拟图的遍历过程，并为每个变量维护一个还有多少其他变量需要它的计数。当我们发现某个变量的计数变成0时，我们便回收其内存空间。 co-share, 我们允许两个变量使用同一段内存空间。这么做当然会使得这两个变量不能同时在写这段空间。所以我们只考虑对不能并行的变量进行co-share。每一次我们考虑图中的一条路（path），路上所有变量都有依赖关系所以不能被并行，然后我们对其进行内存分配并将它们从图中删掉。 引擎在MXNet中，所有的任务，包括张量计算，symbol执行，数据通讯，都会交由引擎来执行。首先，所有的资源单元，例如NDArray，随机数生成器，和临时空间，都会在引擎处注册一个唯一的标签。然后每个提交给引擎的任务都会标明它所需要的资源标签。引擎则会跟踪每个资源，如果某个任务所需要的资源到到位了，例如产生这个资源的上一个任务已经完成了，那么引擎会则调度和执行这个任务。 通常一个MXNet运行实例会使用多个硬件资源，包括CPU，GPU，PCIe通道，网络，和磁盘，所以引擎会使用多线程来调度，既任何两个没有资源依赖冲突的任务都可能会被并行执行，以求最大化资源利用。 与通常的数据流引擎不同的是，MXNet的引擎允许一个任务修改现有的资源。为了保证调度正确性，提交任务时需要分开标明哪些资源是只读，哪些资源会被修改。这个附加的写依赖可以带来很多便利。例如我们可以方便实现在numpy以及其他张量库中常见的数组修改操作，同时也使得内存分配时更加容易，比如操作子可以修改其内部状态变量而不需要每次都重来内存。再次，假如我们要用同一个种子生成两个随机数，那么我们可以标注这两个操作会同时修改种子来使得引擎不会并行执行，从而使得代码的结果可以很好的被重复。 数据通讯KVStore的实现是基于参数服务器。但它跟前面的工作有两个显著的区别。 我们通过引擎来管理数据一致性，这使得参数服务器的实现变得相当简单，同时使得KVStore的运算可以无缝的与其他结合在一起。 我们使用一个两层的通讯结构，原理如下图所示。第一层的服务器管理单机内部的多个设备之间的通讯。第二层服务器则管理机器之间通过网络的通讯。第一层的服务器在与第二层通讯前可能合并设备之间的数据来降低网络带宽消费。同时考虑到机器内和外通讯带宽和延时的不同性，我们可以对其使用不同的一致性模型。例如第一层我们用强的一致性模型，而第二层我们则使用弱的一致性模型来减少同步开销。 可移植性轻量和可移植性是MXNet的一个重要目标。MXNet核心使用C++实现，并提供C风格的头文件。因此方便系统移植，也使得其很容易被其他支持C FFI (forigen language interface )的语言调用。此外，我们也提供一个脚本将MXNet核心功能的代码连同所有依赖打包成一个单一的只有数万行的C++源文件，使得其在一些受限的平台，例如智能设备，方便编译和使用。 实验结果这里我们提供一些早期的实验结果。 与其他系统相比我们首先使用一个流行卷积网络测试方案来对比MXNet与Torch，Caffe和TensorFlow在过去几届imagenet竞赛冠军网络上的性能。每个系统使用同样的CUDA 7.0和CUDNN 3，但TensorFlow使用其只支持的CUDA 6.5 和CUDNN 2。我们使用单块GTX 980并报告单个forward和backward的耗时。 可以看出MXNet，Torch和Caffe三者在性能上不相上下。这个符合预期，因为在单卡上我们评测的几个网络的绝大部分运算都由CUDA和CUDNN完成。TensorFlow比其他三者都慢2倍以上，这可能由于是低版本的CUDNN和项目刚开源的缘故。 #####内存的使用接下来我们考察不同的内存分配算法对内存占用的影响。下图分别表示使用batch=128时，在做预测时和做训练时的不同算法在内部变量（除去模型，最初输入和最终输出）上的内存开销。 可以看出，inplace和co-share两者都可以极大的降低内存使用。将两者合起来可以在训练时减少2倍内存使用，在预测时则可以减小4倍内存使用。特别的，即使是最复杂的vggnet，对单张图片进行预测时，MXNet只需要16MB额外内存。 Scalability最后我们报告在分布式训练下的性能。我们使用imagenet 1k数据（120万224x224x3图片，1000类），并用googlenet加上batch normalization来训练。我们使用Amazon EC2 g2.8x，单机和多机均使用同样的参数，下图表示了使用单机和10台g2.8x时的收敛情况。 从训练精度来看，单机的收敛比多机快，这个符合预期，因为多机时有效的batch大小比单机要大，在处理同样多的数据上收敛通常会慢。但有意思的是两者在测试精度上非常相似。 单机下每遍历一次数据需要1万4千秒，而在十台机器上，每次只需要1千4百秒。如果考虑运行时间对比测试精度，10台机器带来了10倍的提升。 过去，现状，和未来大半年前我们拉来数个优秀的C++机器学习系统的开发人员成立了DMLC，本意是更方便共享各自项目的代码，并给用户提供一致的体验。当时我们有两个深度学习的项目，一个是CXXNet，其通过配置来定义和训练神经网络。另一个是Minerva，提供类似numpy一样的张量计算接口。前者在图片分类等使用卷积网络上很方便，而后者更灵活。那时候我们想要能不能一个两者功能都具备的系统，于是这样就有了MXNet。其名字来自Minerva的M和CXXNet的XNet。其中Symbol的想法来自CXXNet，而NDArray的想法来自Minerva。我们也常把MXNet叫“mix net”。 MXNet是DMLC第一个结合了所有的成员努力的项目，也同时吸引了很多核心成员的加入。MXNet目的是做一个有意思的系统，能够让大家用着方便的系统，一个轻量的和可以快速测试系统和算法想法的系统。对于未来，我们主要关注下面四个方向： 支持更多的硬件，我们目前在积极考虑支持AMD GPU，高通GPU，Intel Phi，FPGA，和更多智能设备。相信MXNet的轻量和内存节省可以在这些上大有作为。 更加完善的操作子。目前不论是Symbol还是NDArray支持的操作还是有限，我们希望能够尽快的扩充他们。 更多编程语言。除了C++，目前MXNet对Python，R和Julia的支持比较完善。但我们希望还能有很多的语言，例如javascript。 更的应用。我们之前花了很多精力在图片分类上，下面我们会考虑很多的应用。 原文：https://github.com/dmlc/mxnet/issues/797论文：mxnet-learningsys]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Introduction to XGBoost]]></title>
      <url>%2F2016%2F12%2F06%2FIntroduction-to-XGBoost%2F</url>
      <content type="text"><![CDATA[eXtreme Gradient Boosting, 它是Gradient Boosting Machine的一个c++实现，同时提供python/R的接口，作者为机器学习的大牛 - 陈天奇。 xgboost的全称是eXtreme Gradient Boosting。正如其名，它是Gradient Boosting Machine的一个c++实现，同时提供python/R的接口，作者为机器学习的大牛陈天奇。 github - dmlc/xgboost XGBoost: A Scalable Tree Boosting System Boosted Tree简介Boosted Tree是数据挖掘和机器学习中最常用的算法之一。它非常有效，对输入要求不敏感，无需进行归一化等较复杂操作，同时作为一种常用的非线性模型，学习能力很强。它是kaggle比赛冠军选手最常用的工具，也在工业界中有大量的应用。 Boosted Tree有各种马甲，比如GBDT, GBRT (gradient boosted regression tree)，MART(Multiple Additive Regression Tree) ，LambdaMART(Learning to Rank模型)也是一种boosted tree的变种。 基学习器：分类和回归树（CART）首先讲模型，Boosted tree 最基本的组成部分叫做回归树(regression tree)，也叫做CART(Classification And Regression Tree)。CART会把输入样本根据不同的特征分配到各个叶子节点，而每个叶子节点上面都会对应一个实数分数。 上面的例子是一个预测一个人是否会喜欢电脑游戏的 CART，你可以把叶子的分数理解为有多可能这个人喜欢电脑游戏。CART算法是一种 回归树，与决策树不同，它的叶子节点从简单的类别变成分值，可以处理，如概率预测，排序等问题。 Tree Ensemble一个CART往往过于简单无法有效地预测，因此一个更加强力的模型叫做tree ensemble。 在上面的例子中，我们用两棵树来进行预测的例子。我们对于每个样本的预测结果就是每棵树预测分数的和。 Tree Ensemble算法被广泛应用，例如GBM、随机森林等。对输入变量的缩放不敏感，不需要做归一化。可扩展，数据挖掘比赛和工业界都在使用。 XGBoost简介xgboost是属于dmlc(Distributed (Deep) Machine Learning Community) 开源项目下的一个子项目，是大规模并行boosted tree的工具，它是目前最快最好的开源boosted tree工具包，比常见的工具包快10倍以上。可用于分类、回归和排序，有大量kaggle选手选用它进行数据挖掘比赛，在工业界规模方面，xgboost的分布式版本有广泛的可移植性，支持在YARN, MPI, Sungrid Engine等各个平台上面运行，并且保留了单机并行版本的各种优化，使得它可以很好地解决于工业界规模的问题。与GBDT相比，xgboost训练速度更快，可扩展性更高。实测20节点训练33G数据(9,200,000 * 400)，建100棵深度为6的树，纯计算时间18分钟。 数学推导对于tree ensemble，假设我们已经有K颗CART树，对于一个输入样本xi，我们通过下面公式得到它的预测值： $$ \overline{y_i} = \sum^K_{k=1}f_k(x_i), \qquad f_x \in F $$ 目标函数：给出我们需要优化的目标函数： $$Obj = \sum^n_{i=1}l(y_i, \overline{y_i}) + \sum^K_{k=1}\Omega(f_k)$$ 其中第一部分是训练误差，也就是大家相对比较熟悉的如平方误差, logistic loss等。而第二部分是每棵树的复杂度的和，可以是：L2、L1、树叶子个数等。当我讨论建立一颗决策树时，一般会考虑到利用信息增益拆分节点、对树进行剪枝、限制最大深度等优化算法。这些策略背后隐藏着这一个目标函数，它们可以很好的对应到这个目标函数上： 信息增益 $\rightarrow$ 损失函数 减枝 $\rightarrow$ 正则化部分结点个数 最大深度 $\rightarrow$ 限制树规模 理解目标函数本身也有利于我们学习和设计算法，公示中第一部分是损失函数，第二部分是树的复杂度。 现在我们的参数可以认为是在一个函数空间里面，我们不能利用传统的随机梯度下降法SGD来学习我们的模型，因此我们采用了一种叫additive training (boosting)的方式，每次保留原来的模型不变，只是加入一个新的函数到模型中。 选取的新的函数要使目标函数尽量降低。 例如：我们拿平方误差作为损失函数，这时目标函数可以写成下面的形式： 对于一般情况，我们无法像平方误差一样展开计算，xgboost采用了二阶泰勒展开公式来近似定义的目标函数，方便我们计算， （2） 当我们把常数项移除之后，我们会发现如下一个比较统一的目标函数。这一个目标函数有一个非常明显的特点，它只依赖于每个数据点在损失函数上的一阶导数和二阶导数。 这样的好处是，模型和代码分离开来，通过替换不同的损失函数模型，算法可以求解包括回归、分类、排序等机器学习问题。 树的复杂度：首先定义函数$f_t(x)$，其中$W$代表树叶的权重，$q$表示一个映射关系，即树的结构。对于给定的$x$通过$q$映射到叶子节点，从而得到分值$w$。 $$f_t(x) = w_q(x),w\in R^T,q:R^d \rightarrow {1,2,…,T}$$ xgboost对树的复杂度定义：这个复杂度包含了一棵树里面叶子节点的个数，以及每个树叶子节点上面输出分数的$L2$模平方。lambda是L2正则化项的惩罚系数，提高此值使算法更加保守。gamma可以理解为每次节点分割的增益的最小值。 改写目标函数：首先定义$I_j$，表示第$i$个叶子节点上样本的集合。 $$I_j = {i|q(x_i)=j}$$ $$Obj^{(t)} \approx \sum^n_{i=1} [g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)] + \Omega(f_t) //二阶泰勒展开$$ $$Obj_{(t)} \approx \sum^t_{i=1}[g_iw_{q(x_i)}+\frac{1}{2}h_iw^2_{q(x_i)}] + \gamma T + \lambda /frac{1}{2}\sum^T_{j=1}w^2_j //展开正则化项$$ $$Obj_{(t)} \approx \sum^T_{j=1}[(\sum_{i\in I_j}g_i)w_j +\frac{1}{2}(\sum_{i \in I_J}h_i + \lambda )w^2_j] + \gamma T //更改遍历方式$$ 经过变换，这一目标包含$T$个独立的单变量函数，将对$n$个样本点的遍历转换成对$T$个叶结点的遍历。进一步定义 $$G_j = \sum_{i \in I_j}g_j \qquad H_j = \sum_{i \in I_j}h_i$$ $Gj, Hj$表示叶结点样本点对应的一阶、二阶导数之和。目标函数可以进一步改写成： $$Obj^{(t)} = \sum^T_{j=1}[(\sum_{i \in I_j} g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i + \lambda)w^2_j] + \gamma T = \sum^T_{j=1}[G_jw_j + \frac{1}{2}(H_j + \lambda)w^2_j] + \gamma T$$ 当树的结构已经确定，我们可以通过简单的求导得到每个叶结点的最优权值和对应的最优目标函数值。 $$w^*_j = - \frac {G_J}{H_j + \lambda} \qquad Obj = - \frac{1}{2} \sum^T_{j=1} \frac{G^2_j}{H_j + \lambda} + \gamma T$$ 我们可以把目标函数叫做结构分数(structure score)。你可以认为这个就是类似吉尼系数一样更加一般的对于树结构进行打分的函数。下面是一个具体的打分函数计算的例子 利用贪心法建树：枚举所有的树形结构是不现实的，一般采用贪心法。利用打分函数来寻找出一个最优结构的树，每一次尝试去对已有的叶子加入一个分割。对于一个具体的分割方案，我们的目的是使目标函数降低的最多，用当前的目标函数 - 分割之后左右子树的目标函数分值，获得的增益可以由如下公式计算： 观察这个公式，我们会发现一个值得注意的事情就是引入分割不一定会使得情况变好，因为我们也会引入新叶子的惩罚项$\gamma$。$\underline{这个目标对应了的剪枝}$， 当引入的分割带来的增益小于一个阀值$\gamma$的时候，我们可以剪掉这个分割。大家可以发现，当我们正式地推导目标的时候，像计算分数和剪枝这样的策略都会自然地出现，而不再是一种因为heuristic而进行的操作了。 xgboost特点总结xgboost是GB算法的高效实现，xgboost中的基学习器除了可以是CART（gbtree）也可以是线性分类器（gblinear）。 xgboost算法的步骤和GB基本相同，都是首先初始化为一个常数，GB是根据一阶导数$ri$，xgboost是根据一阶导数$gi$和二阶导数$hi$，迭代生成基学习器，相加更新学习器。 (1). xgboost在目标函数中显示的加上了正则化项，基学习为CART时，正则化项与树的叶子节点的数量T和叶子节点的值有关。 (2). GB中使用Loss Function对$f(x)$的一阶导数计算出伪残差用于学习生成$fm(x)$，xgboost不仅使用到了一阶导数，还使用二阶导数。 第t次的loss： 对上式做二阶泰勒展开：$g$为一阶导数，$h$为二阶导数 (3). 上面提到CART回归树中寻找最佳分割点的衡量标准是最小化均方差，xgboost寻找分割点的标准是最大化Lsplit，$\lambda$，$\gamma$与正则化项相关 对于某个特定的分割a我们要计算a左边和右边的导数和。我们也可以发现对于所有的a，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和$GL$和$GR$。然后用上面的公式计算每个分割方案的分数就可以了。 (4). xgboost与gdbt除了上述三点的不同，xgboost在实现时还做了许多优化： 在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。 xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。 特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。 按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。 xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。]]></content>
    </entry>

    
  
  
</search>
